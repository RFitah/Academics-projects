---
title: "MRR Project: Variables analysis"
author: "Fitahiry RAJAOBELINA & Dorart RAMADANI"
date: "2023-11-20"
output:
  pdf_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: 72
---

## Introduction

The dataset that we will use all along the project is named "Breast
cancer Winsconsin (Diagnostic)" and is associated to health and medecine
subject. It is a multivariate dataset (569 observations of 30
variables), all the features values are numerical except the feature
"diagnosis". The main information that the dataset is containing is the
diagnosis of breast cancer, saying that a person (identified by the
column "ID") is positive or not (in the column "diagnosis") to breast
cancer according to plenty of features. Features are computed from a
digitized image of a fine needle aspirate (FNA) of a breast mass. They
describe characteristics of the cell nuclei present in the image.

```{r}
# Dataset dimension 
data_df <- read.csv(file="breast-cancer.csv", header=TRUE, sep=",")
data_df <- data_df[,2:32] #Removing ID
dim(data_df)
```

The dataset is fully exploitable because it has no missing values.

```{r}
# Searching for missing values
sum(is.na(data_df))
```

```{r}
head(data_df)
```

## Variables description

As we stated just before, the dataset contain information on cancer
diagnosis, which is related to other variables of the breast mass such
as radius mean, smoothness mean, concavity worst, etc.

### The target variable: Diagnosis

The columns "diagnosis" contain the values M (for malign) and B (for
benign). Here's a barplot which sum up the number of benign and malign
in the dataset, nearly 350 person are benign where 200 are malign.

```{r}
table_diag <- table(data_df$diagnosis) 
barplot(table_diag)
```

### Explicative variables

The final goal of the project is to predict if a breast mass is benign
of malign considering its caracteristics, thoses caracteristics are all
the columns in the dataset except the column "Id" and the column
"diagnosis".

#### Statisticals indicators

We compute some statistical indicators of the explicative variables.

```{r}
#Extract the explicative variables
X <- data_df[,2:31] 

library(psych)

#Table containing stat indicators on all variables
variables_descritpion <- describe(X) 
head(variables_descritpion)
```

#### Histogram

The histogram of some variables can be plotted as the following (the
mean is the red line):

```{r}
par(mfrow=c(1,3))

hist(X$radius_mean, main="Histogram of radius mean", xlab="radius mean" )
abline(v=14.13, col="red")
hist(X$texture_mean, main="Histogram of texture mean", xlab="texture mean" )
abline(v=19.29, col="red")
hist(X$compactness_se, main="Histogram of compactness se", xlab="compactness se" )
abline(v=0.03, col="red")

par(mfrow=c(1,1))
```

We can observe that the majority the variables have their values
gathered in a certain intervall, some histograms are even approaching a
bell curve (texture mean for instance).

#### Relation to cancer diagnosis

To vizualize the relationship between the features and the target
variable, scatter plots are created. Each feature is plotted against the
target variable, with different marker colors representing benign or
malignant tumors. These visualizations help in understanding the
distribution of feature values and potential separability between the
two classes.

```{r}
par(mfrow=c(1,2))

plot(data_df$radius_mean, data_df$perimeter_mean, col=ifelse(data_df$diagnosis == "M", "orange", "blue"), xlab="radius_mean", ylab="perimeter_mean")
plot(data_df$radius_mean, data_df$texture_mean, col=ifelse(data_df$diagnosis == "M", "orange", "blue"), xlab="radius_mean", ylab="texture_mean")

par(mfrow=c(1,1))


```

```{r}
par(mfrow=c(1,2))

plot(data_df$radius_mean, data_df$concavity_mean, col=ifelse(data_df$diagnosis == "M", "orange", "blue"), xlab="radius_mean", ylab="concavity_mean")
plot(data_df$radius_mean, data_df$smoothness_mean, col=ifelse(data_df$diagnosis == "M", "orange", "blue"), xlab="radius_mean", ylab="smoothness_mean")

par(mfrow=c(1,1))
```

Those plots shows that a classification is possible to predict the
cancer diagnosis knowing the breast mass caracteristics.

#### Understanding correlation between variables

Correlation between variables has a predictive power, it helps to
predict how changes in one variable will affect changes in another
variable. We can used it as a starting point for investigating causal
relationships between variables.

We plot the correlation matrix between the variables in the features,
obtaining the following:

```{r}
library(corrplot)
cor_matrix <- cor(X)
corrplot(cor_matrix, method="color", tl.cex=0.7)
```

As the features contain so many variables, it is difficult to clearly
see which variables have higher or lower correlation. So we use the
following script to extract low correlation and high correlation index
in the correlation matrix:

```{r}
which(cor_matrix < -0.4, arr.ind=TRUE)
```

```{r}
which(cor_matrix < -0.3, arr.ind=TRUE)
```

```{r}
which(cor_matrix < -0.2, arr.ind=TRUE)
```

The first phenomen we observe is that all the values have their
correlation greater than $-0.4$. The variables which have the lowest
correlation ratio between them is the variable "fractal_dimension_mean"
and "radius_mean"

```{r}
which(cor_matrix > 0.95, arr.ind=TRUE)
```

Here some variables which have a correlation ratio greater than $0.9$: -
radius_mean/perimeter_mean - radius_mean/area_mean -
radius_mean/radius_worst - radius_mean/perimeter_worst -
radius_mean/area_worst - texture_worst/texture_mean -
area_mean/radius_worst - area_mean/perimeter_worst -
area_mean/area_worst

The high positive correlation between those variable can be explain
geometrically. As those variables are geometric indicators of the breast
mass, it is clear that their values are strongly related.



## Diagnosis prediction (using different methods) 

### Logistic regression model using L2-regularization (Ridge)

```{r}

library(glmnet)


# Step 1: Data Preprocessing
# Separating features and target variable
X <- as.matrix(data_df[, -1]) # Features as matrix 
y <- ifelse(data_df[, 1] == "M", 1, 0) # Target variable: Malignant (1) or Benign (0)

# Step 2: Model Building - Ridge Logistic Regression
# Fit the model
ridge_model <- glmnet(X, y, family = "binomial", alpha = 0) # alpha = 0 for Ridge


```

#### First model evaluation



```{r}
# Step 3: Model Evaluation (using cross-validation)
# Cross-validation
cv_fit <- cv.glmnet(X, y, family = "binomial", alpha = 0) 

# Plot the cross-validation results
plot(cv_fit) 
```




```{r}
print(cv_fit)
```

#### Performance of the best model


```{r}
# Get the best lambda value from cross-validation
best_lambda <- cv_fit$lambda.min

# Retrain the model using the best lambda
final_model <- glmnet(X, y, family = "binomial", alpha = 0, lambda = best_lambda)
```


```{r}


# Predictions using the model on the entire dataset
predictions <- predict(final_model, newx = X, type = "response", s = best_lambda)
predicted_classes <- ifelse(predictions > 0.5, 1, 0)

# Confusion matrix
conf_matrix <- table(Actual = y, Predicted = predicted_classes)
print(conf_matrix)



```

```{r}
# Given values from the confusion matrix
TP <- 202  # True Positives
FP <- 1    # False Positives
FN <- 10   # False Negatives

# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("Accuracy:", accuracy))


# Calculating precision, recall, and F1-score
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
F1_score <- 2 * (precision * recall) / (precision + recall)


print(paste("Precision:", precision))
print(paste("Recall:", recall))
print(paste("F1-score:", F1_score))

```


### Variable selection (Statistical approach)

#### Stepwise logistic regression

```{r}
model <- glm(y ~ ., family = binomial, data = data_df)

# Perform stepwise logistic regression
stepwise_model <- step(model, direction = "both", trace = FALSE)

# Summary of the selected model
summary(stepwise_model)

```


#### PCA

```{r}

library(caret)
library(glmnet)

# Scale the data
scaled_X <- scale(X)

# Calculate covariance matrix
cov_matrix <- cov(scaled_X)

# Perform eigenvalue decomposition
eigen_values <- eigen(cov_matrix)$values
eigen_vectors <- eigen(cov_matrix)$vectors

# Choose the number of principal components based on variance explained
desired_variance <- 0.95
cumulative_variance <- cumsum(eigen_values) / sum(eigen_values)
n_components <- which(cumulative_variance >= desired_variance)[1]

# Retain the selected number of principal components
selected_vectors <- eigen_vectors[, 1:n_components]
X_pca <- as.matrix(scaled_X) %*% selected_vectors


# Combine PCA components with the target variable
data_pca <- data.frame(X_pca, y)

# Fit logistic regression with glmnet
logistic_model <- cv.glmnet(as.matrix(data_pca[, -ncol(data_pca)]), data_pca$y, family = "binomial")

# View model coefficients
coef(logistic_model)



```


#### Model performance

```{r}

# Predict on the training data
predictions <- predict(logistic_model, newx = as.matrix(data_pca[, -ncol(data_pca)]), type = "response")
predicted_classes <- ifelse(predictions > 0.5, 1, 0)

# Create the confusion matrix
conf_matrix <- table(Actual = data_pca$y, Predicted = predicted_classes)
conf_matrix

# Calculate accuracy, precision, recall, and F1-score
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
recall <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)

cat("Accuracy:", accuracy, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")

```







### K-NN

```{r}
# Splitting the data into training and testing sets
trainIndex <- createDataPartition(data_df$diagnosis, p = 0.7, list = FALSE)
data_train <- data_df[trainIndex, ]
data_test <- data_df[-trainIndex, ]

# Define a function to calculate Euclidean distance between two points
euclidean_distance <- function(x1, x2) {
  sum((x1 - x2)^2)^0.5
}

# Define the KNN function
knn_predict <- function(train_data, test_data, train_labels, k) {
  predictions <- c()
  
  for (i in 1:nrow(test_data)) {
    distances <- c()
    
    for (j in 1:nrow(train_data)) {
      distances <- c(distances, euclidean_distance(test_data[i,], train_data[j,]))
    }
    
    sorted_indices <- order(distances)
    k_nearest_labels <- train_labels[sorted_indices[1:k]]
    
    # Make prediction based on the most frequent label among the k nearest neighbors
    prediction <- names(sort(table(k_nearest_labels), decreasing = TRUE))[1]
    predictions <- c(predictions, prediction)
  }
  
  return(predictions)
}

# Selecting features (assuming you want to use all columns except the diagnosis)
features <- names(data_df)[-which(names(data_df) == "diagnosis")]

# Training and predicting using the KNN function
k <- 5
predictions <- knn_predict(data_train[, features], data_test[, features], data_train$diagnosis, k)


```



```{r}
# Calculating confusion matrix
conf_matrix <- table(predictions, data_test$diagnosis)
print("Confusion Matrix:")
print(conf_matrix)

# Calculating accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)


# Calculating precision
precision <- diag(conf_matrix) / rowSums(conf_matrix)


# Calculating recall (sensitivity)
recall <- diag(conf_matrix) / colSums(conf_matrix)


# Calculating F1 score
f1_score <- 2 * (precision * recall) / (precision + recall)



cat("Accuracy:", accuracy, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
```




